{
  "count": 164,
  "next": "https://paperswithcode.com/api/v1/authors/yann-lecun/papers/?page=2",
  "previous": null,
  "results": [
    {
      "id": "v-jepa-2-self-supervised-video-models-enable",
      "arxiv_id": "2506.09985",
      "nips_id": null,
      "url_abs": "https://arxiv.org/abs/2506.09985v1",
      "url_pdf": "https://arxiv.org/pdf/2506.09985v1.pdf",
      "title": "V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction and Planning",
      "abstract": "A major challenge for modern AI is to learn to understand the world and learn to act largely by observation. This paper explores a self-supervised approach that combines internet-scale video data with a small amount of interaction data (robot trajectories), to develop models capable of understanding, predicting, and planning in the physical world. We first pre-train an action-free joint-embedding-predictive architecture, V-JEPA 2, on a video and image dataset comprising over 1 million hours of internet video. V-JEPA 2 achieves strong performance on motion understanding (77.3 top-1 accuracy on Something-Something v2) and state-of-the-art performance on human action anticipation (39.7 recall-at-5 on Epic-Kitchens-100) surpassing previous task-specific models. Additionally, after aligning V-JEPA 2 with a large language model, we demonstrate state-of-the-art performance on multiple video question-answering tasks at the 8 billion parameter scale (e.g., 84.0 on PerceptionTest, 76.9 on TempCompass). Finally, we show how self-supervised learning can be applied to robotic planning tasks by post-training a latent action-conditioned world model, V-JEPA 2-AC, using less than 62 hours of unlabeled robot videos from the Droid dataset. We deploy V-JEPA 2-AC zero-shot on Franka arms in two different labs and enable picking and placing of objects using planning with image goals. Notably, this is achieved without collecting any data from the robots in these environments, and without any task-specific training or reward. This work demonstrates how self-supervised learning from web-scale data and a small amount of robot interaction data can yield a world model capable of planning in the physical world.",
      "authors": [
        "Nicolas Ballas",
        "Michael Rabbat",
        "Yann Lecun",
        "Franziska Meier",
        "Sarath Chandar",
        "Xiaodong Ma",
        "Yong Li",
        "Kapil Krishnakumar",
        "Marc Szafraniec",
        "Francisco Massa",
        "Patrick Labatut",
        "Vasil Khalidov",
        "Piotr Bojanowski",
        "Daniel Dugas",
        "Francois Robert Hogan",
        "Ada Martin",
        "Abha Gejji",
        "Sergio Arnaud",
        "Artem Zholus",
        "Koustuv Sinha",
        "Claire Roberts",
        "Ammar Rizvi",
        "Matthew Muckley",
        "Komeili",
        "Mojtaba",
        "Russell Howes",
        "Quentin Garrido",
        "David Fan",
        "Adrien Bardes",
        "Mido Assran"
      ],
      "published": "2025-06-11",
      "conference": null,
      "conference_url_abs": null,
      "conference_url_pdf": null,
      "proceeding": null
    },
    {
      "id": "from-tokens-to-thoughts-how-llms-and-humans",
      "arxiv_id": "2505.17117",
      "nips_id": null,
      "url_abs": "https://arxiv.org/abs/2505.17117v2",
      "url_pdf": "https://arxiv.org/pdf/2505.17117v2.pdf",
      "title": "From Tokens to Thoughts: How LLMs and Humans Trade Compression for Meaning",
      "abstract": "Humans organize knowledge into compact categories through semantic compression by mapping diverse instances to abstract representations while preserving meaning (e.g., robin and blue jay are both birds; most birds can fly). These concepts reflect a trade-off between expressive fidelity and representational simplicity. Large Language Models (LLMs) demonstrate remarkable linguistic abilities, yet whether their internal representations strike a human-like trade-off between compression and semantic fidelity is unclear. We introduce a novel information-theoretic framework, drawing from Rate-Distortion Theory and the Information Bottleneck principle, to quantitatively compare these strategies. Analyzing token embeddings from a diverse suite of LLMs against seminal human categorization benchmarks, we uncover key divergences. While LLMs form broad conceptual categories that align with human judgment, they struggle to capture the fine-grained semantic distinctions crucial for human understanding. More fundamentally, LLMs demonstrate a strong bias towards aggressive statistical compression, whereas human conceptual systems appear to prioritize adaptive nuance and contextual richness, even if this results in lower compressional efficiency by our measures. These findings illuminate critical differences between current AI and human cognitive architectures, guiding pathways toward LLMs with more human-aligned conceptual representations.",
      "authors": [
        "Ravid Shwartz-Ziv",
        "Yann Lecun",
        "Dan Jurafsky",
        "Chen Shani"
      ],
      "published": "2025-05-21",
      "conference": null,
      "conference_url_abs": null,
      "conference_url_pdf": null,
      "proceeding": null
    },
    {
      "id": "transformers-without-normalization",
      "arxiv_id": "2503.10622",
      "nips_id": null,
      "url_abs": "https://arxiv.org/abs/2503.10622v1",
      "url_pdf": "https://arxiv.org/pdf/2503.10622v1.pdf",
      "title": "Transformers without Normalization",
      "abstract": "Normalization layers are ubiquitous in modern neural networks and have long been considered essential. This work demonstrates that Transformers without normalization can achieve the same or better performance using a remarkably simple technique. We introduce Dynamic Tanh (DyT), an element-wise operation $DyT($x$) = \\tanh(\\alpha $x$)$, as a drop-in replacement for normalization layers in Transformers. DyT is inspired by the observation that layer normalization in Transformers often produces tanh-like, $S$-shaped input-output mappings. By incorporating DyT, Transformers without normalization can match or exceed the performance of their normalized counterparts, mostly without hyperparameter tuning. We validate the effectiveness of Transformers with DyT across diverse settings, ranging from recognition to generation, supervised to self-supervised learning, and computer vision to language models. These findings challenge the conventional understanding that normalization layers are indispensable in modern neural networks, and offer new insights into their role in deep networks.",
      "authors": [
        "Zhuang Liu",
        "Yann Lecun",
        "Kaiming He",
        "Xinlei Chen",
        "Jiachen Zhu"
      ],
      "published": "2025-03-13",
      "conference": null,
      "conference_url_abs": "http://openaccess.thecvf.com//content/CVPR2025/html/Zhu_Transformers_without_Normalization_CVPR_2025_paper.html",
      "conference_url_pdf": "http://openaccess.thecvf.com//content/CVPR2025/papers/Zhu_Transformers_without_Normalization_CVPR_2025_paper.pdf",
      "proceeding": "cvpr-2025-1"
    },
    {
      "id": "forgotten-polygons-multimodal-large-language",
      "arxiv_id": "2502.15969",
      "nips_id": null,
      "url_abs": "https://arxiv.org/abs/2502.15969v1",
      "url_pdf": "https://arxiv.org/pdf/2502.15969v1.pdf",
      "title": "Forgotten Polygons: Multimodal Large Language Models are Shape-Blind",
      "abstract": "Despite strong performance on vision-language tasks, Multimodal Large Language Models (MLLMs) struggle with mathematical problem-solving, with both open-source and state-of-the-art models falling short of human performance on visual-math benchmarks. To systematically examine visual-mathematical reasoning in MLLMs, we (1) evaluate their understanding of geometric primitives, (2) test multi-step reasoning, and (3) explore a potential solution to improve visual reasoning capabilities. Our findings reveal fundamental shortcomings in shape recognition, with top models achieving under 50% accuracy in identifying regular polygons. We analyze these failures through the lens of dual-process theory and show that MLLMs rely on System 1 (intuitive, memorized associations) rather than System 2 (deliberate reasoning). Consequently, MLLMs fail to count the sides of both familiar and novel shapes, suggesting they have neither learned the concept of sides nor effectively process visual inputs. Finally, we propose Visually Cued Chain-of-Thought (VC-CoT) prompting, which enhances multi-step mathematical reasoning by explicitly referencing visual annotations in diagrams, boosting GPT-4o's accuracy on an irregular polygon side-counting task from 7% to 93%. Our findings suggest that System 2 reasoning in MLLMs remains an open problem, and visually-guided prompting is essential for successfully engaging visual reasoning. Code available at: https://github.com/rsinghlab/Shape-Blind.",
      "authors": [
        "Ritambhara Singh",
        "Carsten Eickhoff",
        "Yann Lecun",
        "Vedant Palit",
        "Amir Bar",
        "Michal Golovanesky",
        "William Rudman"
      ],
      "published": "2025-02-21",
      "conference": null,
      "conference_url_abs": null,
      "conference_url_pdf": null,
      "proceeding": null
    },
    {
      "id": "learning-from-reward-free-offline-data-a-case",
      "arxiv_id": "2502.14819",
      "nips_id": null,
      "url_abs": "https://arxiv.org/abs/2502.14819v1",
      "url_pdf": "https://arxiv.org/pdf/2502.14819v1.pdf",
      "title": "Learning from Reward-Free Offline Data: A Case for Planning with Latent Dynamics Models",
      "abstract": "A long-standing goal in AI is to build agents that can solve a variety of tasks across different environments, including previously unseen ones. Two dominant approaches tackle this challenge: (i) reinforcement learning (RL), which learns policies through trial and error, and (ii) optimal control, which plans actions using a learned or known dynamics model. However, their relative strengths and weaknesses remain underexplored in the setting where agents must learn from offline trajectories without reward annotations. In this work, we systematically analyze the performance of different RL and control-based methods under datasets of varying quality. On the RL side, we consider goal-conditioned and zero-shot approaches. On the control side, we train a latent dynamics model using the Joint Embedding Predictive Architecture (JEPA) and use it for planning. We study how dataset properties-such as data diversity, trajectory quality, and environment variability-affect the performance of these approaches. Our results show that model-free RL excels when abundant, high-quality data is available, while model-based planning excels in generalization to novel environment layouts, trajectory stitching, and data-efficiency. Notably, planning with a latent dynamics model emerges as a promising approach for zero-shot generalization from suboptimal data.",
      "authors": [
        "Yann Lecun",
        "Tim G. J. Rudner",
        "Randall Balestriero",
        "Kynghyun Cho",
        "Wancong Zhang",
        "Vlad Sobal"
      ],
      "published": "2025-02-20",
      "conference": null,
      "conference_url_abs": null,
      "conference_url_pdf": null,
      "proceeding": null
    },
    {
      "id": "intuitive-physics-understanding-emerges-from",
      "arxiv_id": "2502.11831",
      "nips_id": null,
      "url_abs": "https://arxiv.org/abs/2502.11831v1",
      "url_pdf": "https://arxiv.org/pdf/2502.11831v1.pdf",
      "title": "Intuitive physics understanding emerges from self-supervised pretraining on natural videos",
      "abstract": "We investigate the emergence of intuitive physics understanding in general-purpose deep neural network models trained to predict masked regions in natural videos. Leveraging the violation-of-expectation framework, we find that video prediction models trained to predict outcomes in a learned representation space demonstrate an understanding of various intuitive physics properties, such as object permanence and shape consistency. In contrast, video prediction in pixel space and multimodal large language models, which reason through text, achieve performance closer to chance. Our comparisons of these architectures reveal that jointly learning an abstract representation space while predicting missing parts of sensory input, akin to predictive coding, is sufficient to acquire an understanding of intuitive physics, and that even models trained on one week of unique video achieve above chance performance. This challenges the idea that core knowledge -- a set of innate systems to help understand the world -- needs to be hardwired to develop an understanding of intuitive physics.",
      "authors": [
        "Yann Lecun",
        "Emmanuel Dupoux",
        "Michael Rabbat",
        "Laurent Najman",
        "Adrien Bardes",
        "Mahmoud Assran",
        "Nicolas Ballas",
        "Quentin Garrido"
      ],
      "published": "2025-02-17",
      "conference": null,
      "conference_url_abs": null,
      "conference_url_pdf": null,
      "proceeding": null
    },
    {
      "id": "layer-by-layer-uncovering-hidden",
      "arxiv_id": "2502.02013",
      "nips_id": null,
      "url_abs": "https://arxiv.org/abs/2502.02013v1",
      "url_pdf": "https://arxiv.org/pdf/2502.02013v1.pdf",
      "title": "Layer by Layer: Uncovering Hidden Representations in Language Models",
      "abstract": "From extracting features to generating text, the outputs of large language models (LLMs) typically rely on their final layers, following the conventional wisdom that earlier layers capture only low-level cues. However, our analysis shows that intermediate layers can encode even richer representations, often improving performance on a wide range of downstream tasks. To explain and quantify these hidden-layer properties, we propose a unified framework of representation quality metrics based on information theory, geometry, and invariance to input perturbations. Our framework highlights how each model layer balances information compression and signal preservation, revealing why mid-depth embeddings can exceed the last layer's performance. Through extensive experiments on 32 text-embedding tasks and comparisons across model architectures (transformers, state-space models) and domains (language, vision), we demonstrate that intermediate layers consistently provide stronger features. These findings challenge the standard focus on final-layer embeddings and open new directions for model analysis and optimization, including strategic use of mid-layer representations for more robust and accurate AI systems.",
      "authors": [
        "Ravid Shwartz-Ziv",
        "Yann Lecun",
        "Jalal Naghiyev",
        "Niket Patel",
        "Dan Zhao",
        "Md Rifat Arefin",
        "Oscar Skean"
      ],
      "published": "2025-02-04",
      "conference": null,
      "conference_url_abs": null,
      "conference_url_pdf": null,
      "proceeding": null
    },
    {
      "id": "metamorph-multimodal-understanding-and",
      "arxiv_id": "2412.14164",
      "nips_id": null,
      "url_abs": "https://arxiv.org/abs/2412.14164v1",
      "url_pdf": "https://arxiv.org/pdf/2412.14164v1.pdf",
      "title": "MetaMorph: Multimodal Understanding and Generation via Instruction Tuning",
      "abstract": "In this work, we propose Visual-Predictive Instruction Tuning (VPiT) - a simple and effective extension to visual instruction tuning that enables a pretrained LLM to quickly morph into an unified autoregressive model capable of generating both text and visual tokens. VPiT teaches an LLM to predict discrete text tokens and continuous visual tokens from any input sequence of image and text data curated in an instruction-following format. Our empirical investigation reveals several intriguing properties of VPiT: (1) visual generation ability emerges as a natural byproduct of improved visual understanding, and can be unlocked efficiently with a small amount of generation data; (2) while we find understanding and generation to be mutually beneficial, understanding data contributes to both capabilities more effectively than generation data. Building upon these findings, we train our MetaMorph model and achieve competitive performance on both visual understanding and generation. In visual generation, MetaMorph can leverage the world knowledge and reasoning abilities gained from LLM pretraining, and overcome common failure modes exhibited by other generation models. Our results suggest that LLMs may have strong \"prior\" vision capabilities that can be efficiently adapted to both visual understanding and generation with a relatively simple instruction tuning process.",
      "authors": [
        "Zhuang Liu",
        "Saining Xie",
        "Yann Lecun",
        "Michael Rabbat",
        "Koustuv Sinha",
        "Xinlei Chen",
        "Yunyang Xiong",
        "Jiachen Zhu",
        "David Fan",
        "Shengbang Tong"
      ],
      "published": "2024-12-18",
      "conference": null,
      "conference_url_abs": null,
      "conference_url_pdf": null,
      "proceeding": null
    },
    {
      "id": "video-representation-learning-with-joint",
      "arxiv_id": "2412.10925",
      "nips_id": null,
      "url_abs": "https://arxiv.org/abs/2412.10925v1",
      "url_pdf": "https://arxiv.org/pdf/2412.10925v1.pdf",
      "title": "Video Representation Learning with Joint-Embedding Predictive Architectures",
      "abstract": "Video representation learning is an increasingly important topic in machine learning research. We present Video JEPA with Variance-Covariance Regularization (VJ-VCR): a joint-embedding predictive architecture for self-supervised video representation learning that employs variance and covariance regularization to avoid representation collapse. We show that hidden representations from our VJ-VCR contain abstract, high-level information about the input data. Specifically, they outperform representations obtained from a generative baseline on downstream tasks that require understanding of the underlying dynamics of moving objects in the videos. Additionally, we explore different ways to incorporate latent variables into the VJ-VCR framework that capture information about uncertainty in the future in non-deterministic settings.",
      "authors": [
        "Yann Lecun",
        "Ravid Shwartz-Ziv",
        "Katrina Drozdov"
      ],
      "published": "2024-12-14",
      "conference": null,
      "conference_url_abs": null,
      "conference_url_pdf": null,
      "proceeding": null
    },
    {
      "id": "does-representation-matter-exploring",
      "arxiv_id": "2412.09563",
      "nips_id": null,
      "url_abs": "https://arxiv.org/abs/2412.09563v1",
      "url_pdf": "https://arxiv.org/pdf/2412.09563v1.pdf",
      "title": "Does Representation Matter? Exploring Intermediate Layers in Large Language Models",
      "abstract": "Understanding what defines a good representation in large language models (LLMs) is fundamental to both theoretical understanding and practical applications. In this paper, we investigate the quality of intermediate representations in various LLM architectures, including Transformers and State Space Models (SSMs). We find that intermediate layers often yield more informative representations for downstream tasks than the final layers. To measure the representation quality, we adapt and apply a suite of metrics - such as prompt entropy, curvature, and augmentation-invariance - originally proposed in other contexts. Our empirical study reveals significant architectural differences, how representations evolve throughout training, and how factors like input randomness and prompt length affect each layer. Notably, we observe a bimodal pattern in the entropy of some intermediate layers and consider potential explanations tied to training data. Overall, our results illuminate the internal mechanics of LLMs and guide strategies for architectural optimization and training.",
      "authors": [
        "Ravid Shwartz-Ziv",
        "Yann Lecun",
        "Md Rifat Arefin",
        "Oscar Skean"
      ],
      "published": "2024-12-12",
      "conference": null,
      "conference_url_abs": null,
      "conference_url_pdf": null,
      "proceeding": null
    },
    {
      "id": "rate-in-information-driven-adaptive-dropout",
      "arxiv_id": "2412.07169",
      "nips_id": null,
      "url_abs": "https://arxiv.org/abs/2412.07169v3",
      "url_pdf": "https://arxiv.org/pdf/2412.07169v3.pdf",
      "title": "Rate-In: Information-Driven Adaptive Dropout Rates for Improved Inference-Time Uncertainty Estimation",
      "abstract": "Accurate uncertainty estimation is crucial for deploying neural networks in risk-sensitive applications such as medical diagnosis. Monte Carlo Dropout is a widely used technique for approximating predictive uncertainty by performing stochastic forward passes with dropout during inference. However, using static dropout rates across all layers and inputs can lead to suboptimal uncertainty estimates, as it fails to adapt to the varying characteristics of individual inputs and network layers. Existing approaches optimize dropout rates during training using labeled data, resulting in fixed inference-time parameters that cannot adjust to new data distributions, compromising uncertainty estimates in Monte Carlo simulations. In this paper, we propose Rate-In, an algorithm that dynamically adjusts dropout rates during inference by quantifying the information loss induced by dropout in each layer's feature maps. By treating dropout as controlled noise injection and leveraging information-theoretic principles, Rate-In adapts dropout rates per layer and per input instance without requiring ground truth labels. By quantifying the functional information loss in feature maps, we adaptively tune dropout rates to maintain perceptual quality across diverse medical imaging tasks and architectural configurations. Our extensive empirical study on synthetic data and real-world medical imaging tasks demonstrates that Rate-In improves calibration and sharpens uncertainty estimates compared to fixed or heuristic dropout rates without compromising predictive performance. Rate-In offers a practical, unsupervised, inference-time approach to optimizing dropout for more reliable predictive uncertainty estimation in critical applications.",
      "authors": [
        "John A. Onofrey",
        "Lawrence H. Staib",
        "Yann Lecun",
        "Ravid Shwartz-Ziv",
        "Tal Zeevi"
      ],
      "published": "2024-12-10",
      "conference": null,
      "conference_url_abs": "http://openaccess.thecvf.com//content/CVPR2025/html/Zeevi_Rate-In_Information-Driven_Adaptive_Dropout_Rates_for_Improved_Inference-Time_Uncertainty_Estimation_CVPR_2025_paper.html",
      "conference_url_pdf": "http://openaccess.thecvf.com//content/CVPR2025/papers/Zeevi_Rate-In_Information-Driven_Adaptive_Dropout_Rates_for_Improved_Inference-Time_Uncertainty_Estimation_CVPR_2025_paper.pdf",
      "proceeding": "cvpr-2025-1"
    },
    {
      "id": "navigation-world-models",
      "arxiv_id": "2412.03572",
      "nips_id": null,
      "url_abs": "https://arxiv.org/abs/2412.03572v2",
      "url_pdf": "https://arxiv.org/pdf/2412.03572v2.pdf",
      "title": "Navigation World Models",
      "abstract": "Navigation is a fundamental skill of agents with visual-motor capabilities. We introduce a Navigation World Model (NWM), a controllable video generation model that predicts future visual observations based on past observations and navigation actions. To capture complex environment dynamics, NWM employs a Conditional Diffusion Transformer (CDiT), trained on a diverse collection of egocentric videos of both human and robotic agents, and scaled up to 1 billion parameters. In familiar environments, NWM can plan navigation trajectories by simulating them and evaluating whether they achieve the desired goal. Unlike supervised navigation policies with fixed behavior, NWM can dynamically incorporate constraints during planning. Experiments demonstrate its effectiveness in planning trajectories from scratch or by ranking trajectories sampled from an external policy. Furthermore, NWM leverages its learned visual priors to imagine trajectories in unfamiliar environments from a single input image, making it a flexible and powerful tool for next-generation navigation systems.",
      "authors": [
        "Yann Lecun",
        "Trevor Darrell",
        "Danny Tran",
        "Gaoyue Zhou",
        "Amir Bar"
      ],
      "published": "2024-12-04",
      "conference": null,
      "conference_url_abs": "http://openaccess.thecvf.com//content/CVPR2025/html/Bar_Navigation_World_Models_CVPR_2025_paper.html",
      "conference_url_pdf": "http://openaccess.thecvf.com//content/CVPR2025/papers/Bar_Navigation_World_Models_CVPR_2025_paper.pdf",
      "proceeding": "cvpr-2025-1"
    },
    {
      "id": "robopepp-vision-based-robot-pose-and-joint",
      "arxiv_id": "2411.17662",
      "nips_id": null,
      "url_abs": "https://arxiv.org/abs/2411.17662v2",
      "url_pdf": "https://arxiv.org/pdf/2411.17662v2.pdf",
      "title": "RoboPEPP: Vision-Based Robot Pose and Joint Angle Estimation through Embedding Predictive Pre-Training",
      "abstract": "Vision-based pose estimation of articulated robots with unknown joint angles has applications in collaborative robotics and human-robot interaction tasks. Current frameworks use neural network encoders to extract image features and downstream layers to predict joint angles and robot pose. While images of robots inherently contain rich information about the robot's physical structures, existing methods often fail to leverage it fully; therefore, limiting performance under occlusions and truncations. To address this, we introduce RoboPEPP, a method that fuses information about the robot's physical model into the encoder using a masking-based self-supervised embedding-predictive architecture. Specifically, we mask the robot's joints and pre-train an encoder-predictor model to infer the joints' embeddings from surrounding unmasked regions, enhancing the encoder's understanding of the robot's physical model. The pre-trained encoder-predictor pair, along with joint angle and keypoint prediction networks, is then fine-tuned for pose and joint angle estimation. Random masking of input during fine-tuning and keypoint filtering during evaluation further improves robustness. Our method, evaluated on several datasets, achieves the best results in robot pose and joint angle estimation while being the least sensitive to occlusions and requiring the lowest execution time.",
      "authors": [
        "Farshad Khorrami",
        "Yann Lecun",
        "Prashanth Krishnamurthy",
        "Raktim Gautam Goswami"
      ],
      "published": "2024-11-26",
      "conference": null,
      "conference_url_abs": "http://openaccess.thecvf.com//content/CVPR2025/html/Goswami_RoboPEPP_Vision-Based_Robot_Pose_and_Joint_Angle_Estimation_through_Embedding_CVPR_2025_paper.html",
      "conference_url_pdf": "http://openaccess.thecvf.com//content/CVPR2025/papers/Goswami_RoboPEPP_Vision-Based_Robot_Pose_and_Joint_Angle_Estimation_through_Embedding_CVPR_2025_paper.pdf",
      "proceeding": "cvpr-2025-1"
    },
    {
      "id": "improving-pre-trained-self-supervised",
      "arxiv_id": "2411.15931",
      "nips_id": null,
      "url_abs": "https://arxiv.org/abs/2411.15931v2",
      "url_pdf": "https://arxiv.org/pdf/2411.15931v2.pdf",
      "title": "Improving Pre-trained Self-Supervised Embeddings Through Effective Entropy Maximization",
      "abstract": "A number of different architectures and loss functions have been applied to the problem of self-supervised learning (SSL), with the goal of developing embeddings that provide the best possible pre-training for as-yet-unknown, lightly supervised downstream tasks. One of these SSL criteria is to maximize the entropy of a set of embeddings in some compact space. But the goal of maximizing the embedding entropy often depends -- whether explicitly or implicitly -- upon high dimensional entropy estimates, which typically perform poorly in more than a few dimensions. In this paper, we motivate an effective entropy maximization criterion (E2MC), defined in terms of easy-to-estimate, low-dimensional constraints. We demonstrate that using it to continue training an already-trained SSL model for only a handful of epochs leads to a consistent and, in some cases, significant improvement in downstream performance. We perform careful ablation studies to show that the improved performance is due to the proposed add-on criterion. We also show that continued pre-training with alternative criteria does not lead to notable improvements, and in some cases, even degrades performance.",
      "authors": [
        "Erik Learned-Miller",
        "Tim G. J. Rudner",
        "Yann Lecun",
        "Deep Chakraborty"
      ],
      "published": "2024-11-24",
      "conference": null,
      "conference_url_abs": null,
      "conference_url_pdf": null,
      "proceeding": null
    },
    {
      "id": "dino-wm-world-models-on-pre-trained-visual",
      "arxiv_id": "2411.04983",
      "nips_id": null,
      "url_abs": "https://arxiv.org/abs/2411.04983v2",
      "url_pdf": "https://arxiv.org/pdf/2411.04983v2.pdf",
      "title": "DINO-WM: World Models on Pre-trained Visual Features enable Zero-shot Planning",
      "abstract": "The ability to predict future outcomes given control actions is fundamental for physical reasoning. However, such predictive models, often called world models, remains challenging to learn and are typically developed for task-specific solutions with online policy learning. To unlock world models' true potential, we argue that they should 1) be trainable on offline, pre-collected trajectories, 2) support test-time behavior optimization, and 3) facilitate task-agnostic reasoning. To this end, we present DINO World Model (DINO-WM), a new method to model visual dynamics without reconstructing the visual world. DINO-WM leverages spatial patch features pre-trained with DINOv2, enabling it to learn from offline behavioral trajectories by predicting future patch features. This allows DINO-WM to achieve observational goals through action sequence optimization, facilitating task-agnostic planning by treating goal features as prediction targets. We demonstrate that DINO-WM achieves zero-shot behavioral solutions at test time on six environments without expert demonstrations, reward modeling, or pre-learned inverse models, outperforming prior state-of-the-art work across diverse task families such as arbitrarily configured mazes, push manipulation with varied object shapes, and multi-particle scenarios.",
      "authors": [
        "Lerrel Pinto",
        "Yann Lecun",
        "Hengkai Pan",
        "Gaoyue Zhou"
      ],
      "published": "2024-11-07",
      "conference": null,
      "conference_url_abs": null,
      "conference_url_pdf": null,
      "proceeding": null
    },
    {
      "id": "seq-vcr-preventing-collapse-in-intermediate",
      "arxiv_id": "2411.02344",
      "nips_id": null,
      "url_abs": "https://arxiv.org/abs/2411.02344v1",
      "url_pdf": "https://arxiv.org/pdf/2411.02344v1.pdf",
      "title": "Seq-VCR: Preventing Collapse in Intermediate Transformer Representations for Enhanced Reasoning",
      "abstract": "Decoder-only Transformers often struggle with complex reasoning tasks, particularly arithmetic reasoning requiring multiple sequential operations. In this work, we identify representation collapse in the model's intermediate layers as a key factor limiting their reasoning capabilities. To address this, we propose Sequential Variance-Covariance Regularization (Seq-VCR), which enhances the entropy of intermediate representations and prevents collapse. Combined with dummy pause tokens as substitutes for chain-of-thought (CoT) tokens, our method significantly improves performance in arithmetic reasoning problems. In the challenging $5 \\times 5$ integer multiplication task, our approach achieves $99.5\\%$ exact match accuracy, outperforming models of the same size (which yield $0\\%$ accuracy) and GPT-4 with five-shot CoT prompting ($44\\%$). We also demonstrate superior results on arithmetic expression and longest increasing subsequence (LIS) datasets. Our findings highlight the importance of preventing intermediate layer representation collapse to enhance the reasoning capabilities of Transformers and show that Seq-VCR offers an effective solution without requiring explicit CoT supervision.",
      "authors": [
        "Christopher Pal",
        "Ravid Shwartz-Ziv",
        "Irina Rish",
        "Yann Lecun",
        "Nicolas Gontier",
        "Gopeshh Subbaraj",
        "Md Rifat Arefin"
      ],
      "published": "2024-11-04",
      "conference": null,
      "conference_url_abs": null,
      "conference_url_pdf": null,
      "proceeding": null
    },
    {
      "id": "multi-modal-ai-for-comprehensive-breast",
      "arxiv_id": "2410.21256",
      "nips_id": null,
      "url_abs": "https://arxiv.org/abs/2410.21256v2",
      "url_pdf": "https://arxiv.org/pdf/2410.21256v2.pdf",
      "title": "Multi-modal AI for comprehensive breast cancer prognostication",
      "abstract": "Treatment selection in breast cancer is guided by molecular subtypes and clinical characteristics. However, current tools including genomic assays lack the accuracy required for optimal clinical decision-making. We developed a novel artificial intelligence (AI)-based approach that integrates digital pathology images with clinical data, providing a more robust and effective method for predicting the risk of cancer recurrence in breast cancer patients. Specifically, we utilized a vision transformer pan-cancer foundation model trained with self-supervised learning to extract features from digitized H&E-stained slides. These features were integrated with clinical data to form a multi-modal AI test predicting cancer recurrence and death. The test was developed and evaluated using data from a total of 8,161 female breast cancer patients across 15 cohorts originating from seven countries. Of these, 3,502 patients from five cohorts were used exclusively for evaluation, while the remaining patients were used for training. Our test accurately predicted our primary endpoint, disease-free interval, in the five evaluation cohorts (C-index: 0.71 [0.68-0.75], HR: 3.63 [3.02-4.37, p<0.001]). In a direct comparison (n=858), the AI test was more accurate than Oncotype DX, the standard-of-care 21-gene assay, achieving a C-index of 0.67 [0.61-0.74] versus 0.61 [0.49-0.73], respectively. Additionally, the AI test added independent prognostic information to Oncotype DX in a multivariate analysis (HR: 3.11 [1.91-5.09, p<0.001)]). The test demonstrated robust accuracy across major molecular breast cancer subtypes, including TNBC (C-index: 0.71 [0.62-0.81], HR: 3.81 [2.35-6.17, p=0.02]), where no diagnostic tools are currently recommended by clinical guidelines. These results suggest that our AI test improves upon the accuracy of existing prognostic tests, while being applicable to a wider range of patients.",
      "authors": [
        "Simone Soysal",
        "Lina Sojak",
        "Marcus Vetter",
        "Valerie Speirs",
        "Andrew Clayburn",
        "Danielle Rigau",
        "Benjamin Swett",
        "Theodore Hill",
        "Elisa Liu",
        "Frank Yeung",
        "Mohammad Sadic",
        "Nitya Thakore",
        "Anas Saad",
        "Rafic Beydoun",
        "Lina Daoud",
        "Yu Zong",
        "Kangning Liu",
        "Ugur Ozerdem",
        "Zoe Steinsnyder",
        "Freya Schnabel",
        "Khalil Choucair",
        "Ken G. Zeng",
        "Krzysztof J. Geras",
        "Yann Lecun",
        "Lajos Pusztai",
        "Francisco J. Esteva",
        "Adam Brufsky",
        "Su Hyun Lim",
        "Jae Pak Yi",
        "Sun-Young Jun",
        "Carlo Bifulco",
        "Brian D. Piening",
        "Natalie Klar",
        "Arvydas Laurinavicius",
        "Dovile Zilenaite-Petrulaitiene",
        "Victor Angel",
        "Linda Ma Pak",
        "Soo-Hwang Teo",
        "Haslina Makmur",
        "Jia-Wern Pan",
        "Waleed Abdulsattar",
        "Daniel Baumhoer",
        "Carlos Fernandez-Granda",
        "Irina Ostrovnaya",
        "Frederick Howard",
        "Young-Joon Kang",
        "Nancy Chan",
        "Elena Diana Chiru",
        "Jailan Elayoubi",
        "Joseph Cappadona",
        "Jan Witowski"
      ],
      "published": "2024-10-28",
      "conference": null,
      "conference_url_abs": null,
      "conference_url_pdf": null,
      "proceeding": null
    },
    {
      "id": "poodle-pooled-and-dense-self-supervised",
      "arxiv_id": "2408.11208",
      "nips_id": null,
      "url_abs": "https://arxiv.org/abs/2408.11208v2",
      "url_pdf": "https://arxiv.org/pdf/2408.11208v2.pdf",
      "title": "PooDLe: Pooled and dense self-supervised learning from naturalistic videos",
      "abstract": "Self-supervised learning has driven significant progress in learning from single-subject, iconic images. However, there are still unanswered questions about the use of minimally-curated, naturalistic video data, which contain dense scenes with many independent objects, imbalanced class distributions, and varying object sizes. In this paper, we propose PooDLe, a self-supervised learning method that combines an invariance-based objective on pooled representations with a dense SSL objective that enforces equivariance to optical flow warping. Our results show that a unified objective applied at multiple feature scales is essential for learning effective image representations from naturalistic videos. We validate our method with experiments on the BDD100K driving video dataset and the Walking Tours first-person video dataset, demonstrating its ability to capture spatial understanding from a dense objective and semantic understanding via a pooled representation objective.",
      "authors": [
        "Mengye Ren",
        "Yann Lecun",
        "Yuwen Xiong",
        "Christopher Hoang",
        "Alex N. Wang"
      ],
      "published": "2024-08-20",
      "conference": null,
      "conference_url_abs": null,
      "conference_url_pdf": null,
      "proceeding": null
    },
    {
      "id": "mathbb-x-sample-contrastive-loss-improving",
      "arxiv_id": "2407.18134",
      "nips_id": null,
      "url_abs": "https://arxiv.org/abs/2407.18134v2",
      "url_pdf": "https://arxiv.org/pdf/2407.18134v2.pdf",
      "title": "$\\mathbb{X}$-Sample Contrastive Loss: Improving Contrastive Learning with Sample Similarity Graphs",
      "abstract": "Learning good representations involves capturing the diverse ways in which data samples relate. Contrastive loss - an objective matching related samples - underlies methods from self-supervised to multimodal learning. Contrastive losses, however, can be viewed more broadly as modifying a similarity graph to indicate how samples should relate in the embedding space. This view reveals a shortcoming in contrastive learning: the similarity graph is binary, as only one sample is the related positive sample. Crucially, similarities \\textit{across} samples are ignored. Based on this observation, we revise the standard contrastive loss to explicitly encode how a sample relates to others. We experiment with this new objective, called $\\mathbb{X}$-Sample Contrastive, to train vision models based on similarities in class or text caption descriptions. Our study spans three scales: ImageNet-1k with 1 million, CC3M with 3 million, and CC12M with 12 million samples. The representations learned via our objective outperform both contrastive self-supervised and vision-language models trained on the same data across a range of tasks. When training on CC12M, we outperform CLIP by $0.6\\%$ on both ImageNet and ImageNet Real. Our objective appears to work particularly well in lower-data regimes, with gains over CLIP of $16.8\\%$ on ImageNet and $18.1\\%$ on ImageNet Real when training with CC3M. Finally, our objective seems to encourage the model to learn representations that separate objects from their attributes and backgrounds, with gains of $3.3$-$5.6$\\% over CLIP on ImageNet9. We hope the proposed solution takes a small step towards developing richer learning objectives for understanding sample relations in foundation models.",
      "authors": [
        "Yann Lecun",
        "Kyunghyun Cho",
        "Pietro Astolfi",
        "Diane Bouchacourt",
        "Vivien Cabannes",
        "Randall Balestriero",
        "Mark Ibrahim",
        "Vlad Sobal"
      ],
      "published": "2024-07-25",
      "conference": null,
      "conference_url_abs": null,
      "conference_url_pdf": null,
      "proceeding": null
    },
    {
      "id": "livebench-a-challenging-contamination-free",
      "arxiv_id": "2406.19314",
      "nips_id": null,
      "url_abs": "https://arxiv.org/abs/2406.19314v2",
      "url_pdf": "https://arxiv.org/pdf/2406.19314v2.pdf",
      "title": "LiveBench: A Challenging, Contamination-Limited LLM Benchmark",
      "abstract": "Test set contamination, wherein test data from a benchmark ends up in a newer model's training set, is a well-documented obstacle for fair LLM evaluation and can quickly render benchmarks obsolete. To mitigate this, many recent benchmarks crowdsource new prompts and evaluations from human or LLM judges; however, these can introduce significant biases, and break down when scoring hard questions. In this work, we introduce a new benchmark for LLMs designed to be resistant to both test set contamination and the pitfalls of LLM judging and human crowdsourcing. We release LiveBench, the first benchmark that (1) contains frequently-updated questions from recent information sources, (2) scores answers automatically according to objective ground-truth values, and (3) contains a wide variety of challenging tasks, spanning math, coding, reasoning, language, instruction following, and data analysis. To achieve this, LiveBench contains questions that are based on recently-released math competitions, arXiv papers, news articles, and datasets, and it contains harder, contamination-limited versions of tasks from previous benchmarks such as Big-Bench Hard, AMPS, and IFEval. We evaluate many prominent closed-source models, as well as dozens of open-source models ranging from 0.5B to 405B in size. LiveBench is difficult, with top models achieving below 70% accuracy. We release all questions, code, and model answers. Questions are added and updated on a monthly basis, and we release new tasks and harder versions of tasks over time so that LiveBench can distinguish between the capabilities of LLMs as they improve in the future. We welcome community engagement and collaboration for expanding the benchmark tasks and models.",
      "authors": [
        "Willie Neiswanger",
        "Siddartha Naidu",
        "Sandeep Singh Sandha",
        "Shubh-Agrawal",
        "Sreemanti Dey",
        "Micah Goldblum",
        "Tom Goldstein",
        "Yann Lecun",
        "Chinmay Hegde",
        "Khalid Saifullah",
        "Neel Jain",
        "Ravid Shwartz-Ziv",
        "Siddhartha Jain",
        "Ben Feuer",
        "Arka Pal",
        "Manley Roberts",
        "Samuel Dooley",
        "Colin White"
      ],
      "published": "2024-06-27",
      "conference": null,
      "conference_url_abs": null,
      "conference_url_pdf": null,
      "proceeding": null
    },
    {
      "id": "cambrian-1-a-fully-open-vision-centric",
      "arxiv_id": "2406.16860",
      "nips_id": null,
      "url_abs": "https://arxiv.org/abs/2406.16860v2",
      "url_pdf": "https://arxiv.org/pdf/2406.16860v2.pdf",
      "title": "Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs",
      "abstract": "We introduce Cambrian-1, a family of multimodal LLMs (MLLMs) designed with a vision-centric approach. While stronger language models can enhance multimodal capabilities, the design choices for vision components are often insufficiently explored and disconnected from visual representation learning research. This gap hinders accurate sensory grounding in real-world scenarios. Our study uses LLMs and visual instruction tuning as an interface to evaluate various visual representations, offering new insights into different models and architectures -- self-supervised, strongly supervised, or combinations thereof -- based on experiments with over 20 vision encoders. We critically examine existing MLLM benchmarks, address the difficulties involved in consolidating and interpreting results from various tasks, and introduce a new vision-centric benchmark, CV-Bench. To further improve visual grounding, we propose the Spatial Vision Aggregator (SVA), a dynamic and spatially-aware connector that integrates high-resolution vision features with LLMs while reducing the number of tokens. Additionally, we discuss the curation of high-quality visual instruction-tuning data from publicly available sources, emphasizing the importance of data source balancing and distribution ratio. Collectively, Cambrian-1 not only achieves state-of-the-art performance but also serves as a comprehensive, open cookbook for instruction-tuned MLLMs. We provide model weights, code, supporting tools, datasets, and detailed instruction-tuning and evaluation recipes. We hope our release will inspire and accelerate advancements in multimodal systems and visual representation learning.",
      "authors": [
        "Ziteng Wang",
        "Saining Xie",
        "Yann Lecun",
        "Rob Fergus",
        "Xichen Pan",
        "Adithya Iyer",
        "Shusheng Yang",
        "Jihan Yang",
        "Sai Charitha Akula",
        "Manoj Middepogu",
        "Sanghyun Woo",
        "Penghao Wu",
        "Ellis Brown",
        "Shengbang Tong"
      ],
      "published": "2024-06-24",
      "conference": null,
      "conference_url_abs": null,
      "conference_url_pdf": null,
      "proceeding": null
    },
    {
      "id": "just-how-flexible-are-neural-networks-in",
      "arxiv_id": "2406.11463",
      "nips_id": null,
      "url_abs": "https://arxiv.org/abs/2406.11463v1",
      "url_pdf": "https://arxiv.org/pdf/2406.11463v1.pdf",
      "title": "Just How Flexible are Neural Networks in Practice?",
      "abstract": "It is widely believed that a neural network can fit a training set containing at least as many samples as it has parameters, underpinning notions of overparameterized and underparameterized models. In practice, however, we only find solutions accessible via our training procedure, including the optimizer and regularizers, limiting flexibility. Moreover, the exact parameterization of the function class, built into an architecture, shapes its loss surface and impacts the minima we find. In this work, we examine the ability of neural networks to fit data in practice. Our findings indicate that: (1) standard optimizers find minima where the model can only fit training sets with significantly fewer samples than it has parameters; (2) convolutional networks are more parameter-efficient than MLPs and ViTs, even on randomly labeled data; (3) while stochastic training is thought to have a regularizing effect, SGD actually finds minima that fit more training data than full-batch gradient descent; (4) the difference in capacity to fit correctly labeled and incorrectly labeled samples can be predictive of generalization; (5) ReLU activation functions result in finding minima that fit more data despite being designed to avoid vanishing and exploding gradients in deep architectures.",
      "authors": [
        "Andrew Gordon Wilson",
        "Yann Lecun",
        "C. Bayan Bruss",
        "Arpit Bansal",
        "Micah Goldblum",
        "Ravid Shwartz-Ziv"
      ],
      "published": "2024-06-17",
      "conference": null,
      "conference_url_abs": null,
      "conference_url_pdf": null,
      "proceeding": null
    },
    {
      "id": "towards-an-improved-understanding-and",
      "arxiv_id": "2406.09366",
      "nips_id": null,
      "url_abs": "https://arxiv.org/abs/2406.09366v1",
      "url_pdf": "https://arxiv.org/pdf/2406.09366v1.pdf",
      "title": "Towards an Improved Understanding and Utilization of Maximum Manifold Capacity Representations",
      "abstract": "Maximum Manifold Capacity Representations (MMCR) is a recent multi-view self-supervised learning (MVSSL) method that matches or surpasses other leading MVSSL methods. MMCR is intriguing because it does not fit neatly into any of the commonplace MVSSL lineages, instead originating from a statistical mechanical perspective on the linear separability of data manifolds. In this paper, we seek to improve our understanding and our utilization of MMCR. To better understand MMCR, we leverage tools from high dimensional probability to demonstrate that MMCR incentivizes alignment and uniformity of learned embeddings. We then leverage tools from information theory to show that such embeddings maximize a well-known lower bound on mutual information between views, thereby connecting the geometric perspective of MMCR to the information-theoretic perspective commonly discussed in MVSSL. To better utilize MMCR, we mathematically predict and experimentally confirm non-monotonic changes in the pretraining loss akin to double descent but with respect to atypical hyperparameters. We also discover compute scaling laws that enable predicting the pretraining loss as a function of gradients steps, batch size, embedding dimension and number of views. We then show that MMCR, originally applied to image data, is performant on multimodal image-text data. By more deeply understanding the theoretical and empirical behavior of MMCR, our work reveals insights on improving MVSSL methods.",
      "authors": [
        "Sanmi Koyejo",
        "Ravid Shwartz-Ziv",
        "Andrey Gromov",
        "SueYeon Chung",
        "Yann Lecun",
        "Thomas Yerxa",
        "Mikail Khona",
        "Alyssa Unell",
        "Berivan Isik",
        "Andres Carranza",
        "Dhruv Bhandarkar Pai",
        "Victor Lecomte",
        "Rylan Schaeffer"
      ],
      "published": "2024-06-13",
      "conference": null,
      "conference_url_abs": null,
      "conference_url_pdf": null,
      "proceeding": null
    },
    {
      "id": "towards-a-framework-for-openness-in",
      "arxiv_id": "2405.15802",
      "nips_id": null,
      "url_abs": "https://arxiv.org/abs/2405.15802v1",
      "url_pdf": "https://arxiv.org/pdf/2405.15802v1.pdf",
      "title": "Towards a Framework for Openness in Foundation Models: Proceedings from the Columbia Convening on Openness in Artificial Intelligence",
      "abstract": "Over the past year, there has been a robust debate about the benefits and risks of open sourcing foundation models. However, this discussion has often taken place at a high level of generality or with a narrow focus on specific technical attributes. In part, this is because defining open source for foundation models has proven tricky, given its significant differences from traditional software development. In order to inform more practical and nuanced decisions about opening AI systems, including foundation models, this paper presents a framework for grappling with openness across the AI stack. It summarizes previous work on this topic, analyzes the various potential reasons to pursue openness, and outlines how openness varies in different parts of the AI stack, both at the model and at the system level. In doing so, its authors hope to provide a common descriptive framework to deepen a nuanced and rigorous understanding of openness in AI and enable further work around definitions of openness and safety in AI.",
      "authors": [
        "Justine Tunney",
        "Govind Shivkumar",
        "Nik Marda",
        "Stefano Maffulli",
        "Nathan Lambert",
        "Helen King-Turvey",
        "Mark Surman",
        "Yann Lecun",
        "Sayash Kapoor",
        "Merouane Debbah",
        "Brian Behlendorf",
        "Ayah Bdeir",
        "Kevin Bankston",
        "Victor Storchan",
        "Camille Franois",
        "Adrien Basdevant"
      ],
      "published": "2024-05-17",
      "conference": null,
      "conference_url_abs": null,
      "conference_url_pdf": null,
      "proceeding": null
    },
    {
      "id": "hierarchical-world-models-as-visual-whole",
      "arxiv_id": "2405.18418",
      "nips_id": null,
      "url_abs": "https://arxiv.org/abs/2405.18418v3",
      "url_pdf": "https://arxiv.org/pdf/2405.18418v3.pdf",
      "title": "Hierarchical World Models as Visual Whole-Body Humanoid Controllers",
      "abstract": "Whole-body control for humanoids is challenging due to the high-dimensional nature of the problem, coupled with the inherent instability of a bipedal morphology. Learning from visual observations further exacerbates this difficulty. In this work, we explore highly data-driven approaches to visual whole-body humanoid control based on reinforcement learning, without any simplifying assumptions, reward design, or skill primitives. Specifically, we propose a hierarchical world model in which a high-level agent generates commands based on visual observations for a low-level agent to execute, both of which are trained with rewards. Our approach produces highly performant control policies in 8 tasks with a simulated 56-DoF humanoid, while synthesizing motions that are broadly preferred by humans.",
      "authors": [
        "Hao Su",
        "Xiaolong Wang",
        "Yann Lecun",
        "Vlad Sobal",
        "Jyothir S V",
        "Nicklas Hansen"
      ],
      "published": "2024-05-28",
      "conference": null,
      "conference_url_abs": null,
      "conference_url_pdf": null,
      "proceeding": null
    },
    {
      "id": "fine-tuning-large-vision-language-models-as",
      "arxiv_id": "2405.10292",
      "nips_id": null,
      "url_abs": "https://arxiv.org/abs/2405.10292v3",
      "url_pdf": "https://arxiv.org/pdf/2405.10292v3.pdf",
      "title": "Fine-Tuning Large Vision-Language Models as Decision-Making Agents via Reinforcement Learning",
      "abstract": "Large vision-language models (VLMs) fine-tuned on specialized visual instruction-following data have exhibited impressive language reasoning capabilities across various scenarios. However, this fine-tuning paradigm may not be able to efficiently learn optimal decision-making agents in multi-step goal-directed tasks from interactive environments. To address this challenge, we propose an algorithmic framework that fine-tunes VLMs with reinforcement learning (RL). Specifically, our framework provides a task description and then prompts the VLM to generate chain-of-thought (CoT) reasoning, enabling the VLM to efficiently explore intermediate reasoning steps that lead to the final text-based action. Next, the open-ended text output is parsed into an executable action to interact with the environment to obtain goal-directed task rewards. Finally, our framework uses these task rewards to fine-tune the entire VLM with RL. Empirically, we demonstrate that our proposed framework enhances the decision-making capabilities of VLM agents across various tasks, enabling 7b models to outperform commercial models such as GPT4-V or Gemini. Furthermore, we find that CoT reasoning is a crucial component for performance improvement, as removing the CoT reasoning results in a significant decrease in the overall performance of our method.",
      "authors": [
        "Sergey Levine",
        "Yi Ma",
        "Yann Lecun",
        "Saining Xie",
        "Alane Suhr",
        "Yifei Zhou",
        "Shengbang Tong",
        "Jiayi Pan",
        "Zipeng Lin",
        "Hao Bai",
        "Yuexiang Zhai"
      ],
      "published": "2024-05-16",
      "conference": null,
      "conference_url_abs": null,
      "conference_url_pdf": null,
      "proceeding": null
    },
    {
      "id": "the-entropy-enigma-success-and-failure-of",
      "arxiv_id": "2405.05012",
      "nips_id": null,
      "url_abs": "https://arxiv.org/abs/2405.05012v2",
      "url_pdf": "https://arxiv.org/pdf/2405.05012v2.pdf",
      "title": "The Entropy Enigma: Success and Failure of Entropy Minimization",
      "abstract": "Entropy minimization (EM) is frequently used to increase the accuracy of classification models when they're faced with new data at test time. EM is a self-supervised learning method that optimizes classifiers to assign even higher probabilities to their top predicted classes. In this paper, we analyze why EM works when adapting a model for a few steps and why it eventually fails after adapting for many steps. We show that, at first, EM causes the model to embed test images close to training images, thereby increasing model accuracy. After many steps of optimization, EM makes the model embed test images far away from the embeddings of training images, which results in a degradation of accuracy. Building upon our insights, we present a method for solving a practical problem: estimating a model's accuracy on a given arbitrary dataset without having access to its labels. Our method estimates accuracy by looking at how the embeddings of input images change as the model is optimized to minimize entropy. Experiments on 23 challenging datasets show that our method sets the SoTA with a mean absolute error of $5.75\\%$, an improvement of $29.62\\%$ over the previous SoTA on this task. Our code is available at https://github.com/oripress/EntropyEnigma",
      "authors": [
        "Matthias Bethge",
        "Yann Lecun",
        "Ravid Shwartz-Ziv",
        "Ori Press"
      ],
      "published": "2024-05-08",
      "conference": null,
      "conference_url_abs": null,
      "conference_url_pdf": null,
      "proceeding": null
    },
    {
      "id": "advancing-human-centric-ai-for-robust-x-ray",
      "arxiv_id": "2405.01469",
      "nips_id": null,
      "url_abs": "https://arxiv.org/abs/2405.01469v1",
      "url_pdf": "https://arxiv.org/pdf/2405.01469v1.pdf",
      "title": "Advancing human-centric AI for robust X-ray analysis through holistic self-supervised learning",
      "abstract": "AI Foundation models are gaining traction in various applications, including medical fields like radiology. However, medical foundation models are often tested on limited tasks, leaving their generalisability and biases unexplored. We present RayDINO, a large visual encoder trained by self-supervision on 873k chest X-rays. We compare RayDINO to previous state-of-the-art models across nine radiology tasks, from classification and dense segmentation to text generation, and provide an in depth analysis of population, age and sex biases of our model. Our findings suggest that self-supervision allows patient-centric AI proving useful in clinical workflows and interpreting X-rays holistically. With RayDINO and small task-specific adapters, we reach state-of-the-art results and improve generalization to unseen populations while mitigating bias, illustrating the true promise of foundation models: versatility and robustness.",
      "authors": [
        "Maria Vakalopoulou",
        "Marie-Pierre Revel",
        "Maxime Oquab",
        "Matthew Muckley",
        "Yann Lecun",
        "Armand Joulin",
        "Cline Hudelot",
        "Guillaume Chassagnon",
        "Piotr Bojanowski",
        "Tho Moutakanni"
      ],
      "published": "2024-05-02",
      "conference": null,
      "conference_url_abs": null,
      "conference_url_pdf": null,
      "proceeding": null
    },
    {
      "id": "egopet-egomotion-and-interaction-data-from-an",
      "arxiv_id": "2404.09991",
      "nips_id": null,
      "url_abs": "https://arxiv.org/abs/2404.09991v1",
      "url_pdf": "https://arxiv.org/pdf/2404.09991v1.pdf",
      "title": "EgoPet: Egomotion and Interaction Data from an Animal's Perspective",
      "abstract": "Animals perceive the world to plan their actions and interact with other agents to accomplish complex tasks, demonstrating capabilities that are still unmatched by AI systems. To advance our understanding and reduce the gap between the capabilities of animals and AI systems, we introduce a dataset of pet egomotion imagery with diverse examples of simultaneous egomotion and multi-agent interaction. Current video datasets separately contain egomotion and interaction examples, but rarely both at the same time. In addition, EgoPet offers a radically distinct perspective from existing egocentric datasets of humans or vehicles. We define two in-domain benchmark tasks that capture animal behavior, and a third benchmark to assess the utility of EgoPet as a pretraining resource to robotic quadruped locomotion, showing that models trained from EgoPet outperform those trained from prior datasets.",
      "authors": [
        "Trevor Darrell",
        "Amir Globerson",
        "Yann Lecun",
        "Jathushan Rajasegaran",
        "Antonio Loquercio",
        "Danny Tran",
        "Arya Bakhtiar",
        "Amir Bar"
      ],
      "published": "2024-04-15",
      "conference": null,
      "conference_url_abs": null,
      "conference_url_pdf": null,
      "proceeding": null
    },
    {
      "id": "revisiting-feature-prediction-for-learning-1",
      "arxiv_id": "2404.08471",
      "nips_id": null,
      "url_abs": "https://arxiv.org/abs/2404.08471v1",
      "url_pdf": "https://arxiv.org/pdf/2404.08471v1.pdf",
      "title": "Revisiting Feature Prediction for Learning Visual Representations from Video",
      "abstract": "This paper explores feature prediction as a stand-alone objective for unsupervised learning from video and introduces V-JEPA, a collection of vision models trained solely using a feature prediction objective, without the use of pretrained image encoders, text, negative examples, reconstruction, or other sources of supervision. The models are trained on 2 million videos collected from public datasets and are evaluated on downstream image and video tasks. Our results show that learning by predicting video features leads to versatile visual representations that perform well on both motion and appearance-based tasks, without adaption of the model's parameters; e.g., using a frozen backbone. Our largest model, a ViT-H/16 trained only on videos, obtains 81.9% on Kinetics-400, 72.2% on Something-Something-v2, and 77.9% on ImageNet1K.",
      "authors": [
        "Nicolas Ballas",
        "Mahmoud Assran",
        "Yann Lecun",
        "Michael Rabbat",
        "Xinlei Chen",
        "Jean Ponce",
        "Quentin Garrido",
        "Adrien Bardes"
      ],
      "published": "2024-02-15",
      "conference": null,
      "conference_url_abs": null,
      "conference_url_pdf": null,
      "proceeding": "arxiv-preprint-2024-2"
    },
    {
      "id": "learning-and-leveraging-world-models-in",
      "arxiv_id": "2403.00504",
      "nips_id": null,
      "url_abs": "https://arxiv.org/abs/2403.00504v1",
      "url_pdf": "https://arxiv.org/pdf/2403.00504v1.pdf",
      "title": "Learning and Leveraging World Models in Visual Representation Learning",
      "abstract": "Joint-Embedding Predictive Architecture (JEPA) has emerged as a promising self-supervised approach that learns by leveraging a world model. While previously limited to predicting missing parts of an input, we explore how to generalize the JEPA prediction task to a broader set of corruptions. We introduce Image World Models, an approach that goes beyond masked image modeling and learns to predict the effect of global photometric transformations in latent space. We study the recipe of learning performant IWMs and show that it relies on three key aspects: conditioning, prediction difficulty, and capacity. Additionally, we show that the predictive world model learned by IWM can be adapted through finetuning to solve diverse tasks; a fine-tuned IWM world model matches or surpasses the performance of previous self-supervised methods. Finally, we show that learning with an IWM allows one to control the abstraction level of the learned representations, learning invariant representations such as contrastive methods, or equivariant representations such as masked image modelling.",
      "authors": [
        "Yann Lecun",
        "Laurent Najman",
        "Adrien Bardes",
        "Nicolas Ballas",
        "Mahmoud Assran",
        "Quentin Garrido"
      ],
      "published": "2024-03-01",
      "conference": null,
      "conference_url_abs": null,
      "conference_url_pdf": null,
      "proceeding": null
    },
    {
      "id": "variance-covariance-regularization-improves",
      "arxiv_id": "2306.13292",
      "nips_id": null,
      "url_abs": "https://arxiv.org/abs/2306.13292v2",
      "url_pdf": "https://arxiv.org/pdf/2306.13292v2.pdf",
      "title": "Variance-Covariance Regularization Improves Representation Learning",
      "abstract": "Transfer learning plays a key role in advancing machine learning models, yet conventional supervised pretraining often undermines feature transferability by prioritizing features that minimize the pretraining loss. In this work, we adapt a self-supervised learning regularization technique from the VICReg method to supervised learning contexts, introducing Variance-Covariance Regularization (VCReg). This adaptation encourages the network to learn high-variance, low-covariance representations, promoting learning more diverse features. We outline best practices for an efficient implementation of our framework, including applying it to the intermediate representations. Through extensive empirical evaluation, we demonstrate that our method significantly enhances transfer learning for images and videos, achieving state-of-the-art performance across numerous tasks and datasets. VCReg also improves performance in scenarios like long-tail learning and hierarchical classification. Additionally, we show its effectiveness may stem from its success in addressing challenges like gradient starvation and neural collapse. In summary, VCReg offers a universally applicable regularization framework that significantly advances transfer learning and highlights the connection between gradient starvation, neural collapse, and feature transferability.",
      "authors": [
        "Yann Lecun",
        "Ravid Shwartz-Ziv",
        "Katrina Evtimova",
        "Yubei Chen",
        "Jiachen Zhu"
      ],
      "published": "2023-06-23",
      "conference": null,
      "conference_url_abs": null,
      "conference_url_pdf": null,
      "proceeding": null
    },
    {
      "id": "learning-by-reconstruction-produces",
      "arxiv_id": "2402.11337",
      "nips_id": null,
      "url_abs": "https://arxiv.org/abs/2402.11337v1",
      "url_pdf": "https://arxiv.org/pdf/2402.11337v1.pdf",
      "title": "Learning by Reconstruction Produces Uninformative Features For Perception",
      "abstract": "Input space reconstruction is an attractive representation learning paradigm. Despite interpretability of the reconstruction and generation, we identify a misalignment between learning by reconstruction, and learning for perception. We show that the former allocates a model's capacity towards a subspace of the data explaining the observed variance--a subspace with uninformative features for the latter. For example, the supervised TinyImagenet task with images projected onto the top subspace explaining 90\\% of the pixel variance can be solved with 45\\% test accuracy. Using the bottom subspace instead, accounting for only 20\\% of the pixel variance, reaches 55\\% test accuracy. The features for perception being learned last explains the need for long training time, e.g., with Masked Autoencoders. Learning by denoising is a popular strategy to alleviate that misalignment. We prove that while some noise strategies such as masking are indeed beneficial, others such as additive Gaussian noise are not. Yet, even in the case of masking, we find that the benefits vary as a function of the mask's shape, ratio, and the considered dataset. While tuning the noise strategy without knowledge of the perception task seems challenging, we provide first clues on how to detect if a noise strategy is never beneficial regardless of the perception task.",
      "authors": [
        "Yann Lecun",
        "Randall Balestriero"
      ],
      "published": "2024-02-17",
      "conference": null,
      "conference_url_abs": null,
      "conference_url_pdf": null,
      "proceeding": null
    },
    {
      "id": "revisiting-feature-prediction-for-learning",
      "arxiv_id": null,
      "nips_id": null,
      "url_abs": "https://ai.meta.com/research/publications/revisiting-feature-prediction-for-learning-visual-representations-from-video/",
      "url_pdf": "https://scontent-cdg4-2.xx.fbcdn.net/v/t39.2365-6/427986745_768441298640104_1604906292521363076_n.pdf?_nc_cat=103&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=Lpq5IeF5ftUAX9EN6b7&_nc_ht=scontent-cdg4-2.xx&oh=00_AfCFIyd8GMJnqQsG90WY-ccXwWEooa0XgiWXZm06nd1-pw&oe=65D69EB1",
      "title": "Revisiting Feature Prediction for Learning Visual Representations from Video",
      "abstract": "This paper explores feature prediction as a stand-alone objective for unsupervised learning from video and introduces V-JEPA, a collection of vision models trained solely using a feature prediction objective, without the use of pretrained image encoders, text, negative examples, reconstruction, or other sources of supervision. The models are trained on 2 million videos collected from public datasets and are evaluated on downstream image and video tasks. Our results show that learning by predicting video features leads to versatile visual representations that perform well on both motion and appearance-based tasks, without adaption of the models parameters; e.g., using a frozen backbone. Our largest model, a ViT-H/16 trained only on videos, obtains 81.9% on Kinetics-400, 72.2% on Something-Something-v2, and 77.9% on ImageNet1K.",
      "authors": [
        "Nicolas Ballas",
        "Mahmoud Assran",
        "Yann Lecun",
        "Michael Rabbat",
        "Xinlei Chen",
        "Jean Ponce",
        "Quentin Garrido",
        "Adrien Bardes"
      ],
      "published": "2024-02-14",
      "conference": null,
      "conference_url_abs": null,
      "conference_url_pdf": null,
      "proceeding": "arxiv-preprint-2024-2"
    },
    {
      "id": "self-supervised-learning-with-lie-symmetries",
      "arxiv_id": "2307.05432",
      "nips_id": null,
      "url_abs": "https://arxiv.org/abs/2307.05432v2",
      "url_pdf": "https://arxiv.org/pdf/2307.05432v2.pdf",
      "title": "Self-Supervised Learning with Lie Symmetries for Partial Differential Equations",
      "abstract": "Machine learning for differential equations paves the way for computationally efficient alternatives to numerical solvers, with potentially broad impacts in science and engineering. Though current algorithms typically require simulated training data tailored to a given setting, one may instead wish to learn useful information from heterogeneous sources, or from real dynamical systems observations that are messy or incomplete. In this work, we learn general-purpose representations of PDEs from heterogeneous data by implementing joint embedding methods for self-supervised learning (SSL), a framework for unsupervised representation learning that has had notable success in computer vision. Our representation outperforms baseline approaches to invariant tasks, such as regressing the coefficients of a PDE, while also improving the time-stepping performance of neural solvers. We hope that our proposed methodology will prove useful in the eventual development of general-purpose foundation models for PDEs. Code: https://github.com/facebookresearch/SSLForPDEs.",
      "authors": [
        "Bobak T. Kiani",
        "Yann Lecun",
        "Danyal Rehman",
        "Hannah Lawrence",
        "Quentin Garrido",
        "Grgoire Mialon"
      ],
      "published": "2023-07-11",
      "conference": "self-supervised-learning-with-lie-symmetries",
      "conference_url_abs": "https://openreview.net/forum?id=ZULq9QV8rH",
      "conference_url_pdf": "https://openreview.net/pdf?id=ZULq9QV8rH",
      "proceeding": "neurips-2023-11"
    },
    {
      "id": "g-retriever-retrieval-augmented-generation",
      "arxiv_id": "2402.07630",
      "nips_id": null,
      "url_abs": "https://arxiv.org/abs/2402.07630v3",
      "url_pdf": "https://arxiv.org/pdf/2402.07630v3.pdf",
      "title": "G-Retriever: Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering",
      "abstract": "Given a graph with textual attributes, we enable users to `chat with their graph': that is, to ask questions about the graph using a conversational interface. In response to a user's questions, our method provides textual replies and highlights the relevant parts of the graph. While existing works integrate large language models (LLMs) and graph neural networks (GNNs) in various ways, they mostly focus on either conventional graph tasks (such as node, edge, and graph classification), or on answering simple graph queries on small or synthetic graphs. In contrast, we develop a flexible question-answering framework targeting real-world textual graphs, applicable to multiple applications including scene graph understanding, common sense reasoning, and knowledge graph reasoning. Toward this goal, we first develop a Graph Question Answering (GraphQA) benchmark with data collected from different tasks. Then, we propose our G-Retriever method, introducing the first retrieval-augmented generation (RAG) approach for general textual graphs, which can be fine-tuned to enhance graph understanding via soft prompting. To resist hallucination and to allow for textual graphs that greatly exceed the LLM's context window size, G-Retriever performs RAG over a graph by formulating this task as a Prize-Collecting Steiner Tree optimization problem. Empirical evaluations show that our method outperforms baselines on textual graph tasks from multiple domains, scales well with larger graph sizes, and mitigates hallucination.~\\footnote{Our codes and datasets are available at: \\url{https://github.com/XiaoxinHe/G-Retriever}}",
      "authors": [
        "Bryan Hooi",
        "Xavier Bresson",
        "Yann Lecun",
        "Thomas Laurent",
        "Nitesh V. Chawla",
        "Yifei Sun",
        "Yijun Tian",
        "Xiaoxin He"
      ],
      "published": "2024-02-12",
      "conference": null,
      "conference_url_abs": null,
      "conference_url_pdf": null,
      "proceeding": null
    },
    {
      "id": "fast-and-exact-enumeration-of-deep-networks",
      "arxiv_id": "2401.11188",
      "nips_id": null,
      "url_abs": "https://arxiv.org/abs/2401.11188v1",
      "url_pdf": "https://arxiv.org/pdf/2401.11188v1.pdf",
      "title": "Fast and Exact Enumeration of Deep Networks Partitions Regions",
      "abstract": "One fruitful formulation of Deep Networks (DNs) enabling their theoretical study and providing practical guidelines to practitioners relies on Piecewise Affine Splines. In that realm, a DN's input-mapping is expressed as per-region affine mapping where those regions are implicitly determined by the model's architecture and form a partition of their input space. That partition -- which is involved in all the results spanned from this line of research -- has so far only been computed on $2/3$-dimensional slices of the DN's input space or estimated by random sampling. In this paper, we provide the first parallel algorithm that does exact enumeration of the DN's partition regions. The proposed algorithm enables one to finally assess the closeness of the commonly employed approximations methods, e.g. based on random sampling of the DN input space. One of our key finding is that if one is only interested in regions with ``large'' volume, then uniform sampling of the space is highly efficient, but that if one is also interested in discovering the ``small'' regions of the partition, then uniform sampling is exponentially costly with the DN's input space dimension. On the other hand, our proposed method has complexity scaling linearly with input dimension and the number of regions.",
      "authors": [
        "Yann Lecun",
        "Randall Balestriero"
      ],
      "published": "2024-01-20",
      "conference": null,
      "conference_url_abs": null,
      "conference_url_pdf": null,
      "proceeding": null
    },
    {
      "id": "eyes-wide-shut-exploring-the-visual",
      "arxiv_id": "2401.06209",
      "nips_id": null,
      "url_abs": "https://arxiv.org/abs/2401.06209v2",
      "url_pdf": "https://arxiv.org/pdf/2401.06209v2.pdf",
      "title": "Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs",
      "abstract": "Is vision good enough for language? Recent advancements in multimodal models primarily stem from the powerful reasoning abilities of large language models (LLMs). However, the visual component typically depends only on the instance-level contrastive language-image pre-training (CLIP). Our research reveals that the visual capabilities in recent multimodal LLMs (MLLMs) still exhibit systematic shortcomings. To understand the roots of these errors, we explore the gap between the visual embedding space of CLIP and vision-only self-supervised learning. We identify ''CLIP-blind pairs'' - images that CLIP perceives as similar despite their clear visual differences. With these pairs, we construct the Multimodal Visual Patterns (MMVP) benchmark. MMVP exposes areas where state-of-the-art systems, including GPT-4V, struggle with straightforward questions across nine basic visual patterns, often providing incorrect answers and hallucinated explanations. We further evaluate various CLIP-based vision-and-language models and found a notable correlation between visual patterns that challenge CLIP models and those problematic for multimodal LLMs. As an initial effort to address these issues, we propose a Mixture of Features (MoF) approach, demonstrating that integrating vision self-supervised learning features with MLLMs can significantly enhance their visual grounding capabilities. Together, our research suggests visual representation learning remains an open challenge, and accurate visual grounding is crucial for future successful multimodal systems.",
      "authors": [
        "Saining Xie",
        "Yann Lecun",
        "Yi Ma",
        "Yuexiang Zhai",
        "Zhuang Liu",
        "Shengbang Tong"
      ],
      "published": "2024-01-11",
      "conference": null,
      "conference_url_abs": "http://openaccess.thecvf.com//content/CVPR2024/html/Tong_Eyes_Wide_Shut_Exploring_the_Visual_Shortcomings_of_Multimodal_LLMs_CVPR_2024_paper.html",
      "conference_url_pdf": "http://openaccess.thecvf.com//content/CVPR2024/papers/Tong_Eyes_Wide_Shut_Exploring_the_Visual_Shortcomings_of_Multimodal_LLMs_CVPR_2024_paper.pdf",
      "proceeding": "cvpr-2024-1"
    },
    {
      "id": "gradient-based-planning-with-world-models",
      "arxiv_id": "2312.17227",
      "nips_id": null,
      "url_abs": "https://arxiv.org/abs/2312.17227v1",
      "url_pdf": "https://arxiv.org/pdf/2312.17227v1.pdf",
      "title": "Gradient-based Planning with World Models",
      "abstract": "The enduring challenge in the field of artificial intelligence has been the control of systems to achieve desired behaviours. While for systems governed by straightforward dynamics equations, methods like Linear Quadratic Regulation (LQR) have historically proven highly effective, most real-world tasks, which require a general problem-solver, demand world models with dynamics that cannot be easily described by simple equations. Consequently, these models must be learned from data using neural networks. Most model predictive control (MPC) algorithms designed for visual world models have traditionally explored gradient-free population-based optimisation methods, such as Cross Entropy and Model Predictive Path Integral (MPPI) for planning. However, we present an exploration of a gradient-based alternative that fully leverages the differentiability of the world model. In our study, we conduct a comparative analysis between our method and other MPC-based alternatives, as well as policy-based algorithms. In a sample-efficient setting, our method achieves on par or superior performance compared to the alternative approaches in most tasks. Additionally, we introduce a hybrid model that combines policy networks and gradient-based MPC, which outperforms pure policy based methods thereby holding promise for Gradient-based planning with world models in complex real-world tasks.",
      "authors": [
        "Vlad Sobal",
        "Yann Lecun",
        "Siddhartha Jalagam",
        "Jyothir S V"
      ],
      "published": "2023-12-28",
      "conference": null,
      "conference_url_abs": null,
      "conference_url_pdf": null,
      "proceeding": null
    },
    {
      "id": "reverse-engineering-self-supervised-learning-1",
      "arxiv_id": "2305.15614",
      "nips_id": null,
      "url_abs": "https://arxiv.org/abs/2305.15614v2",
      "url_pdf": "https://arxiv.org/pdf/2305.15614v2.pdf",
      "title": "Reverse Engineering Self-Supervised Learning",
      "abstract": "Self-supervised learning (SSL) is a powerful tool in machine learning, but understanding the learned representations and their underlying mechanisms remains a challenge. This paper presents an in-depth empirical analysis of SSL-trained representations, encompassing diverse models, architectures, and hyperparameters. Our study reveals an intriguing aspect of the SSL training process: it inherently facilitates the clustering of samples with respect to semantic labels, which is surprisingly driven by the SSL objective's regularization term. This clustering process not only enhances downstream classification but also compresses the data information. Furthermore, we establish that SSL-trained representations align more closely with semantic classes rather than random classes. Remarkably, we show that learned representations align with semantic classes across various hierarchical levels, and this alignment increases during training and when moving deeper into the network. Our findings provide valuable insights into SSL's representation learning mechanisms and their impact on performance across different sets of classes.",
      "authors": [
        "Yann Lecun",
        "Shai Dekel",
        "Tomer Galanti",
        "Ravid Shwartz-Ziv",
        "Ido Ben-Shaul"
      ],
      "published": "2023-05-24",
      "conference": "reverse-engineering-self-supervised-learning",
      "conference_url_abs": "https://openreview.net/forum?id=NsVEjx6YPd",
      "conference_url_pdf": "https://openreview.net/pdf?id=NsVEjx6YPd",
      "proceeding": "neurips-2023-11"
    },
    {
      "id": "gaia-a-benchmark-for-general-ai-assistants",
      "arxiv_id": "2311.12983",
      "nips_id": null,
      "url_abs": "https://arxiv.org/abs/2311.12983v1",
      "url_pdf": "https://arxiv.org/pdf/2311.12983v1.pdf",
      "title": "GAIA: a benchmark for General AI Assistants",
      "abstract": "We introduce GAIA, a benchmark for General AI Assistants that, if solved, would represent a milestone in AI research. GAIA proposes real-world questions that require a set of fundamental abilities such as reasoning, multi-modality handling, web browsing, and generally tool-use proficiency. GAIA questions are conceptually simple for humans yet challenging for most advanced AIs: we show that human respondents obtain 92\\% vs. 15\\% for GPT-4 equipped with plugins. This notable performance disparity contrasts with the recent trend of LLMs outperforming humans on tasks requiring professional skills in e.g. law or chemistry. GAIA's philosophy departs from the current trend in AI benchmarks suggesting to target tasks that are ever more difficult for humans. We posit that the advent of Artificial General Intelligence (AGI) hinges on a system's capability to exhibit similar robustness as the average human does on such questions. Using GAIA's methodology, we devise 466 questions and their answer. We release our questions while retaining answers to 300 of them to power a leader-board available at https://huggingface.co/gaia-benchmark.",
      "authors": [
        "Thomas Scialom",
        "Yann Lecun",
        "Thomas Wolf",
        "Craig Swift",
        "Clmentine Fourrier",
        "Grgoire Mialon"
      ],
      "published": "2023-11-21",
      "conference": null,
      "conference_url_abs": null,
      "conference_url_pdf": null,
      "proceeding": null
    },
    {
      "id": "urlost-unsupervised-representation-learning",
      "arxiv_id": "2310.04496",
      "nips_id": null,
      "url_abs": "https://arxiv.org/abs/2310.04496v1",
      "url_pdf": "https://arxiv.org/pdf/2310.04496v1.pdf",
      "title": "URLOST: Unsupervised Representation Learning without Stationarity or Topology",
      "abstract": "Unsupervised representation learning has seen tremendous progress but is constrained by its reliance on data modality-specific stationarity and topology, a limitation not found in biological intelligence systems. For instance, human vision processes visual signals derived from irregular and non-stationary sampling lattices yet accurately perceives the geometry of the world. We introduce a novel framework that learns from high-dimensional data lacking stationarity and topology. Our model combines a learnable self-organizing layer, density adjusted spectral clustering, and masked autoencoders. We evaluate its effectiveness on simulated biological vision data, neural recordings from the primary visual cortex, and gene expression datasets. Compared to state-of-the-art unsupervised learning methods like SimCLR and MAE, our model excels at learning meaningful representations across diverse modalities without depending on stationarity or topology. It also outperforms other methods not dependent on these factors, setting a new benchmark in the field. This work represents a step toward unsupervised learning methods that can generalize across diverse high-dimensional data modalities.",
      "authors": [
        "Yubei Chen",
        "Yann Lecun",
        "Bruno Olshausen",
        "Juexiao Zhang",
        "Zeyu Yun"
      ],
      "published": "2023-10-06",
      "conference": null,
      "conference_url_abs": null,
      "conference_url_pdf": null,
      "proceeding": null
    },
    {
      "id": "explanations-as-features-llm-based-features",
      "arxiv_id": "2305.19523",
      "nips_id": null,
      "url_abs": "https://arxiv.org/abs/2305.19523v5",
      "url_pdf": "https://arxiv.org/pdf/2305.19523v5.pdf",
      "title": "Harnessing Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed Graph Representation Learning",
      "abstract": "Representation learning on text-attributed graphs (TAGs) has become a critical research problem in recent years. A typical example of a TAG is a paper citation graph, where the text of each paper serves as node attributes. Initial graph neural network (GNN) pipelines handled these text attributes by transforming them into shallow or hand-crafted features, such as skip-gram or bag-of-words features. Recent efforts have focused on enhancing these pipelines with language models (LMs), which typically demand intricate designs and substantial computational resources. With the advent of powerful large language models (LLMs) such as GPT or Llama2, which demonstrate an ability to reason and to utilize general knowledge, there is a growing need for techniques which combine the textual modelling abilities of LLMs with the structural learning capabilities of GNNs. Hence, in this work, we focus on leveraging LLMs to capture textual information as features, which can be used to boost GNN performance on downstream tasks. A key innovation is our use of explanations as features: we prompt an LLM to perform zero-shot classification, request textual explanations for its decision-making process, and design an LLM-to-LM interpreter to translate these explanations into informative features for downstream GNNs. Our experiments demonstrate that our method achieves state-of-the-art results on well-established TAG datasets, including Cora, PubMed, ogbn-arxiv, as well as our newly introduced dataset, tape-arxiv23. Furthermore, our method significantly speeds up training, achieving a 2.88 times improvement over the closest baseline on ogbn-arxiv. Lastly, we believe the versatility of the proposed method extends beyond TAGs and holds the potential to enhance other tasks involving graph-text data. Our codes and datasets are available at: https://github.com/XiaoxinHe/TAPE.",
      "authors": [
        "Bryan Hooi",
        "Yann Lecun",
        "Adam Perold",
        "Thomas Laurent",
        "Xavier Bresson",
        "Xiaoxin He"
      ],
      "published": "2023-05-31",
      "conference": null,
      "conference_url_abs": null,
      "conference_url_pdf": null,
      "proceeding": null
    },
    {
      "id": "predicting-masked-tokens-in-stochastic",
      "arxiv_id": "2308.00566",
      "nips_id": null,
      "url_abs": "https://arxiv.org/abs/2308.00566v2",
      "url_pdf": "https://arxiv.org/pdf/2308.00566v2.pdf",
      "title": "Stochastic positional embeddings improve masked image modeling",
      "abstract": "Masked Image Modeling (MIM) is a promising self-supervised learning approach that enables learning from unlabeled images. Despite its recent success, learning good representations through MIM remains challenging because it requires predicting the right semantic content in accurate locations. For example, given an incomplete picture of a dog, we can guess that there is a tail, but we cannot determine its exact location. In this work, we propose to incorporate location uncertainty into MIM by using stochastic positional embeddings (StoP). Specifically, we condition the model on stochastic masked token positions drawn from a Gaussian distribution. StoP reduces overfitting to location features and guides the model toward learning features that are more robust to location uncertainties. Quantitatively, StoP improves downstream MIM performance on a variety of downstream tasks, including $+1.7\\%$ on ImageNet linear probing using ViT-B, and $+2.5\\%$ for ViT-H using $1\\%$ of the data.",
      "authors": [
        "Yann Lecun",
        "Amir Globerson",
        "Trevor Darrell",
        "Nicolas Ballas",
        "Pascal Vincent",
        "Mahmoud Assran",
        "Assaf Shocher",
        "Florian Bordes",
        "Amir Bar"
      ],
      "published": "2023-07-31",
      "conference": null,
      "conference_url_abs": null,
      "conference_url_pdf": null,
      "proceeding": null
    },
    {
      "id": "mc-jepa-a-joint-embedding-predictive",
      "arxiv_id": "2307.12698",
      "nips_id": null,
      "url_abs": "https://arxiv.org/abs/2307.12698v1",
      "url_pdf": "https://arxiv.org/pdf/2307.12698v1.pdf",
      "title": "MC-JEPA: A Joint-Embedding Predictive Architecture for Self-Supervised Learning of Motion and Content Features",
      "abstract": "Self-supervised learning of visual representations has been focusing on learning content features, which do not capture object motion or location, and focus on identifying and differentiating objects in images and videos. On the other hand, optical flow estimation is a task that does not involve understanding the content of the images on which it is estimated. We unify the two approaches and introduce MC-JEPA, a joint-embedding predictive architecture and self-supervised learning approach to jointly learn optical flow and content features within a shared encoder, demonstrating that the two associated objectives; the optical flow estimation objective and the self-supervised learning objective; benefit from each other and thus learn content features that incorporate motion information. The proposed approach achieves performance on-par with existing unsupervised optical flow benchmarks, as well as with common self-supervised learning approaches on downstream tasks such as semantic segmentation of images and videos.",
      "authors": [
        "Yann Lecun",
        "Jean Ponce",
        "Adrien Bardes"
      ],
      "published": "2023-07-24",
      "conference": null,
      "conference_url_abs": null,
      "conference_url_pdf": null,
      "proceeding": null
    },
    {
      "id": "introduction-to-latent-variable-energy-based",
      "arxiv_id": "2306.02572",
      "nips_id": null,
      "url_abs": "https://arxiv.org/abs/2306.02572v1",
      "url_pdf": "https://arxiv.org/pdf/2306.02572v1.pdf",
      "title": "Introduction to Latent Variable Energy-Based Models: A Path Towards Autonomous Machine Intelligence",
      "abstract": "Current automated systems have crucial limitations that need to be addressed before artificial intelligence can reach human-like levels and bring new technological revolutions. Among others, our societies still lack Level 5 self-driving cars, domestic robots, and virtual assistants that learn reliable world models, reason, and plan complex action sequences. In these notes, we summarize the main ideas behind the architecture of autonomous intelligence of the future proposed by Yann LeCun. In particular, we introduce energy-based and latent variable models and combine their advantages in the building block of LeCun's proposal, that is, in the hierarchical joint embedding predictive architecture (H-JEPA).",
      "authors": [
        "Yann Lecun",
        "Anna Dawid"
      ],
      "published": "2023-06-05",
      "conference": null,
      "conference_url_abs": null,
      "conference_url_pdf": null,
      "proceeding": null
    },
    {
      "id": "a-cookbook-of-self-supervised-learning",
      "arxiv_id": "2304.12210",
      "nips_id": null,
      "url_abs": "https://arxiv.org/abs/2304.12210v2",
      "url_pdf": "https://arxiv.org/pdf/2304.12210v2.pdf",
      "title": "A Cookbook of Self-Supervised Learning",
      "abstract": "Self-supervised learning, dubbed the dark matter of intelligence, is a promising path to advance machine learning. Yet, much like cooking, training SSL methods is a delicate art with a high barrier to entry. While many components are familiar, successfully training a SSL method involves a dizzying set of choices from the pretext tasks to training hyper-parameters. Our goal is to lower the barrier to entry into SSL research by laying the foundations and latest SSL recipes in the style of a cookbook. We hope to empower the curious researcher to navigate the terrain of methods, understand the role of the various knobs, and gain the know-how required to explore how delicious SSL can be.",
      "authors": [
        "Micah Goldblum",
        "Yann Lecun",
        "Hamed Pirsiavash",
        "Amir Bar",
        "Pierre Fernandez",
        "Quentin Garrido",
        "Jonas Geiping",
        "Andrew Gordon Wilson",
        "Avi Schwarzschild",
        "Yuandong Tian",
        "Gregoire Mialon",
        "Adrien Bardes",
        "Florian Bordes",
        "Tom Goldstein",
        "Shashank Shekhar",
        "Ari Morcos",
        "Vlad Sobal",
        "Mark Ibrahim",
        "Randall Balestriero"
      ],
      "published": "2023-04-24",
      "conference": null,
      "conference_url_abs": null,
      "conference_url_pdf": null,
      "proceeding": null
    },
    {
      "id": "to-compress-or-not-to-compress-self",
      "arxiv_id": "2304.09355",
      "nips_id": null,
      "url_abs": "https://arxiv.org/abs/2304.09355v5",
      "url_pdf": "https://arxiv.org/pdf/2304.09355v5.pdf",
      "title": "To Compress or Not to Compress- Self-Supervised Learning and Information Theory: A Review",
      "abstract": "Deep neural networks excel in supervised learning tasks but are constrained by the need for extensive labeled data. Self-supervised learning emerges as a promising alternative, allowing models to learn without explicit labels. Information theory, and notably the information bottleneck principle, has been pivotal in shaping deep neural networks. This principle focuses on optimizing the trade-off between compression and preserving relevant information, providing a foundation for efficient network design in supervised contexts. However, its precise role and adaptation in self-supervised learning remain unclear. In this work, we scrutinize various self-supervised learning approaches from an information-theoretic perspective, introducing a unified framework that encapsulates the \\textit{self-supervised information-theoretic learning problem}. We weave together existing research into a cohesive narrative, delve into contemporary self-supervised methodologies, and spotlight potential research avenues and inherent challenges. Additionally, we discuss the empirical evaluation of information-theoretic quantities and their estimation methods. Overall, this paper furnishes an exhaustive review of the intersection of information theory, self-supervised learning, and deep neural networks.",
      "authors": [
        "Yann Lecun",
        "Ravid Shwartz-Ziv"
      ],
      "published": "2023-04-19",
      "conference": null,
      "conference_url_abs": null,
      "conference_url_pdf": null,
      "proceeding": null
    },
    {
      "id": "emp-ssl-towards-self-supervised-learning-in",
      "arxiv_id": "2304.03977",
      "nips_id": null,
      "url_abs": "https://arxiv.org/abs/2304.03977v1",
      "url_pdf": "https://arxiv.org/pdf/2304.03977v1.pdf",
      "title": "EMP-SSL: Towards Self-Supervised Learning in One Training Epoch",
      "abstract": "Recently, self-supervised learning (SSL) has achieved tremendous success in learning image representation. Despite the empirical success, most self-supervised learning methods are rather \"inefficient\" learners, typically taking hundreds of training epochs to fully converge. In this work, we show that the key towards efficient self-supervised learning is to increase the number of crops from each image instance. Leveraging one of the state-of-the-art SSL method, we introduce a simplistic form of self-supervised learning method called Extreme-Multi-Patch Self-Supervised-Learning (EMP-SSL) that does not rely on many heuristic techniques for SSL such as weight sharing between the branches, feature-wise normalization, output quantization, and stop gradient, etc, and reduces the training epochs by two orders of magnitude. We show that the proposed method is able to converge to 85.1% on CIFAR-10, 58.5% on CIFAR-100, 38.1% on Tiny ImageNet and 58.5% on ImageNet-100 in just one epoch. Furthermore, the proposed method achieves 91.5% on CIFAR-10, 70.1% on CIFAR-100, 51.5% on Tiny ImageNet and 78.9% on ImageNet-100 with linear probing in less than ten training epochs. In addition, we show that EMP-SSL shows significantly better transferability to out-of-domain datasets compared to baseline SSL methods. We will release the code in https://github.com/tsb0601/EMP-SSL.",
      "authors": [
        "Yann Lecun",
        "Yi Ma",
        "Yubei Chen",
        "Shengbang Tong"
      ],
      "published": "2023-04-08",
      "conference": null,
      "conference_url_abs": null,
      "conference_url_pdf": null,
      "proceeding": null
    },
    {
      "id": "active-self-supervised-learning-a-few-low",
      "arxiv_id": "2303.15256",
      "nips_id": null,
      "url_abs": "https://arxiv.org/abs/2303.15256v2",
      "url_pdf": "https://arxiv.org/pdf/2303.15256v2.pdf",
      "title": "Active Self-Supervised Learning: A Few Low-Cost Relationships Are All You Need",
      "abstract": "Self-Supervised Learning (SSL) has emerged as the solution of choice to learn transferable representations from unlabeled data. However, SSL requires to build samples that are known to be semantically akin, i.e. positive views. Requiring such knowledge is the main limitation of SSL and is often tackled by ad-hoc strategies e.g. applying known data-augmentations to the same input. In this work, we formalize and generalize this principle through Positive Active Learning (PAL) where an oracle queries semantic relationships between samples. PAL achieves three main objectives. First, it unveils a theoretically grounded learning framework beyond SSL, based on similarity graphs, that can be extended to tackle supervised and semi-supervised learning depending on the employed oracle. Second, it provides a consistent algorithm to embed a priori knowledge, e.g. some observed labels, into any SSL losses without any change in the training pipeline. Third, it provides a proper active learning framework yielding low-cost solutions to annotate datasets, arguably bringing the gap between theory and practice of active learning that is based on simple-to-answer-by-non-experts queries of semantic relationships between inputs.",
      "authors": [
        "Randall Balestriero",
        "Yann Lecun",
        "Leon Bottou",
        "Vivien Cabannes"
      ],
      "published": "2023-03-27",
      "conference": null,
      "conference_url_abs": "http://openaccess.thecvf.com//content/ICCV2023/html/Cabannes_Active_Self-Supervised_Learning_A_Few_Low-Cost_Relationships_Are_All_You_ICCV_2023_paper.html",
      "conference_url_pdf": "http://openaccess.thecvf.com//content/ICCV2023/papers/Cabannes_Active_Self-Supervised_Learning_A_Few_Low-Cost_Relationships_Are_All_You_ICCV_2023_paper.pdf",
      "proceeding": "iccv-2023-1"
    }
  ]
}